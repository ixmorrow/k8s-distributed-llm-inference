apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ .Release.Name }}-inference-api
  labels:
    app: llm-inference-api
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      app: llm-inference-api
  template:
    metadata:
      labels:
        app: llm-inference-api
    spec:
      containers:
      - name: inference-api
        image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
        imagePullPolicy: {{ .Values.image.pullPolicy }}
        ports:
        - containerPort: 8000
        resources:
          limits:
            cpu: {{ .Values.resources.limits.cpu }}
            memory: {{ .Values.resources.limits.memory }}
          requests:
            cpu: {{ .Values.resources.requests.cpu }}
            memory: {{ .Values.resources.requests.memory }}