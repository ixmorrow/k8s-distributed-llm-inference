replicaCount: 2
image:
  repository: us-central1-docker.pkg.dev/liquid-layout-449900-m0/llm-repo/llm-inference-api
  tag: latest
  pullPolicy: Always
service:
  type: LoadBalancer
  port: 80
resources:
  limits:
    cpu: "1000m"
    memory: "4Gi"
  requests:
    cpu: "500m"
    memory: "2Gi"
autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 5
  targetCPUUtilizationPercentage: 50
  targetMemoryUtilizationPercentage: 70
  scaleDown:
    stabilizationWindowSeconds: 60
    policies:
      - type: Pods
        value: 1
        periodSeconds: 60